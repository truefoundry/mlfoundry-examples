{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try this Notebook in Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/10FZr5ORlgIo3f2UCg_QoVHmKI0qbh_WS?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Gg-iNkR0BN",
    "tags": []
   },
   "source": [
    "# 🏃‍♂ TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFt8hpFSR0BO",
    "tags": []
   },
   "source": [
    "# 💪 Full example\n",
    "**To demonstrate we will finetune a small bert model on \"emotion\" subset of [Tweet Eval](https://huggingface.co/datasets/tweet_eval) dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHT56iepvJ5D",
    "tags": []
   },
   "source": [
    "## ⬇️ Install dependencies\n",
    "\n",
    "For torch, it is recommended to follow the instructions at https://pytorch.org/get-started/locally/  \n",
    "We will use the one already installed, otherwise we will just install the CPU version for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHSinaERR0BP"
   },
   "outputs": [],
   "source": [
    "! pip install --quiet \"numpy>=1.0.0,<2.0.0\" \"pandas>=1.0.0,<2.0.0\" scikit-learn shap==0.40.0 \"tokenizers<0.13.0\" \"datasets>=2.2.1,<2.3.0\" \"transformers>=4.19.0,<4.20.0\"\n",
    "! pip install --quiet \"torch>=1.2.0,<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2a3EqiQsjxb"
   },
   "outputs": [],
   "source": [
    "! pip freeze | grep 'torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-q6xYFaR0BR",
    "tags": []
   },
   "source": [
    "## 🛠 Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48TsOZzltV7A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPFPQzXKR0BR",
    "tags": []
   },
   "source": [
    "### 📦 Load the dataset and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSrRp35nta3J"
   },
   "outputs": [],
   "source": [
    "TASK = \"classification\"\n",
    "DATASET = \"tweet_eval\"\n",
    "SUBSET = \"emotion\"\n",
    "MODEL_CHECKPOINT = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "\n",
    "dataset = load_dataset(DATASET, SUBSET)\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R27eqx8ttc5k"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdk10OfgtebL"
   },
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlUPnn2gxCjW"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PE56TPqxZL6"
   },
   "outputs": [],
   "source": [
    "sentence1_key, sentence2_key = \"text\", None\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5TzllgnR0BT",
    "tags": []
   },
   "source": [
    "### 🤖 Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJ8JDQMOxg_R"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, max_length=256)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcnwvDTKxios"
   },
   "outputs": [],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li8NDay4xm4X"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8aOqKDZR0BT",
    "tags": []
   },
   "source": [
    "### ⚙️ Load the model checkpoint, setup training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpxUdw43gfwQ"
   },
   "outputs": [],
   "source": [
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "labels = dataset['train'].features['label'].names\n",
    "label2id = dict(zip(labels, range(len(labels))))\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv_P4tf3xs9z"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT, label2id=label2id, id2label=id2label)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXyuBMg7xvJm"
   },
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-{DATASET}-{SUBSET}-{TASK}\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    warmup_ratio=0.3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USYN36NzR0BU",
    "tags": []
   },
   "source": [
    "### ⚡️ Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtEZzWu0x1IY"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRWunNedx3fZ"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe_ivOow112R"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RIYROhM1bRn"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = model.eval()\n",
    "classifier = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Did we miss the fact that #BurkeRamsey swung &amp;hit his sister #JonBenet in the face with a golf club previously out of a fit of ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MnMlQli1_C_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAP568Qy2GAS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tweet_eval_emotion_text_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
