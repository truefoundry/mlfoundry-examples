{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlfoundry as mlf\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_frame = pd.DataFrame(iris.data, columns = iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MlFoundry APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_api = mlf.get_client() # to save locally\n",
    "mlf_run = mlf_api.create_run(project_name='sklearn-project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_run.log_dataset(iris_frame, data_slice=mlf.DataSlice.TRAIN)  # saves in parquet format\n",
    "mlf_run.log_dataset(iris_frame, data_slice=mlf.DataSlice.TEST, fileformat=mlf.FileFormat.CSV) # saves in csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale', kernel='rbf', probability=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'classes': clf.classes_, 'features': clf.n_features_in_}\n",
    "mlf_run.log_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_run.log_model(clf, mlf.ModelFramework.SKLEARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Predictions Synchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = clf.predict(X_train)\n",
    "y_hat_test = clf.predict(X_test)\n",
    "mlf_run.log_predictions(pd.DataFrame(X_test), list(y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Predictions Asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = mlf_run.log_predictions_async(pd.DataFrame(X_test), list(y_hat_test))\n",
    "\n",
    "#### To confirm that the log request completed successfully, await for futures to resolve: This is a blocking call\n",
    "import concurrent.futures as cf\n",
    "for response in cf.as_completed(responses):\n",
    "  res = response.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "metrics_dict = {}\n",
    "\n",
    "metrics_dict['accuracy_score'] = accuracy_score(y_test, y_hat_test)\n",
    "metrics_dict['f1_score'] = f1_score(y_test, y_hat_test, average='micro')\n",
    "\n",
    "mlf_run.log_metrics(metrics_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 120 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Missing config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 25.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "y_train_prob = clf.predict_proba(X_train)\n",
    "\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n",
    "X_train_df['targets'] = y_train\n",
    "X_train_df['predictions'] = y_hat_train\n",
    "X_train_df['prediction_probabilities'] = list(y_train_prob)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n",
    "X_test_df['targets'] = y_test\n",
    "X_test_df['predictions'] = y_hat_test\n",
    "\n",
    "# compute and log stats for train data without shap\n",
    "mlf_run.log_dataset_stats(\n",
    "    X_train_df, \n",
    "    data_slice=mlf.DataSlice.TRAIN,\n",
    "    data_schema=mlf.Schema(\n",
    "        feature_column_names=iris.feature_names,\n",
    "        prediction_column_name=\"predictions\",\n",
    "        actual_column_name=\"targets\",\n",
    "        prediction_probability_column_name=\"prediction_probabilities\"   # to calculate probability related metrics\n",
    "    ),\n",
    "    model_type=mlf.ModelType.MULTICLASS_CLASSIFICATION,\n",
    ")\n",
    "\n",
    "# shap value computation\n",
    "X_train_df1 = pd.DataFrame(X_train, columns=iris.feature_names)\n",
    "X_test_df1 = pd.DataFrame(X_test, columns=iris.feature_names)\n",
    "explainer = shap.KernelExplainer(clf.predict_proba, X_train_df1)\n",
    "shap_values = explainer.shap_values(X_test_df1)\n",
    "\n",
    "mlf_run.log_dataset_stats(\n",
    "    X_test_df, \n",
    "    data_slice=mlf.DataSlice.TEST,\n",
    "    data_schema=mlf.Schema(\n",
    "        feature_column_names=iris.feature_names,\n",
    "        prediction_column_name=\"predictions\",\n",
    "        actual_column_name=\"targets\"\n",
    "    ),\n",
    "    model_type=mlf.ModelType.MULTICLASS_CLASSIFICATION,\n",
    "    shap_values=shap_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8743b0f75b6a55a016638ba229c0d478e77c985bb41c53d6f2fbe781d6f864f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
